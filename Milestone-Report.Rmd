---
title: "Capstone-Milestone-Report"
author: "Kuldeep Singh Meena"
date: "9/4/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is Milestone report part of Data Science Specialization Capstone course. Goal of this project is to develop an application that takes input a word/a phrase and this application will predict next word. Input dataset for this project has been downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip.
Overall, purpose of this assignment is to understand dataset and perform an exploratory data analysis for given files and explain the plan for prediction algorithm that we are going to develop later. Also, use plots and graphs to show exploratory data analysis in effective way.


## Loading Data
``` {r loading Data}

blogs <- readLines("C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.blogs.txt",warn=FALSE,encoding="UTF-8")
news<-readLines("C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.news.txt",warn=FALSE,encoding="UTF-8")
twitter<-readLines("C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.twitter.txt",warn=FALSE,encoding="UTF-8")
```

## Summarizing data
```{r summarize}
size_blogs<-file.size(path="C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.blogs.txt")/2^20
size_news<-file.size(path="C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.news.txt")/2^20
size_twitter<-file.size(path="C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Week2_Assignment/Data/final/en_US/en_US.twitter.txt")/2^20
lnth_blogs<-length(blogs)
lnth_news<-length(news)
lnth_twitter<-length(twitter)
nchar_blogs<-sum(nchar(blogs))
nchar_news<-sum(nchar(news))
nchar_twitter<-sum(nchar(twitter))

library(stringi)
nword_blogs<-stri_stats_latex(blogs)[4]
nword_news<-stri_stats_latex(news)[4]
nword_twitter<-stri_stats_latex(twitter)[4]
table<-data.frame("File Name"=c("Blogs","News","Twitter"),
                  "File Size(MB)"=c(size_blogs,size_news,size_twitter),
                  "Number of rows"=c(lnth_blogs,lnth_news,lnth_twitter),
                  "Number of characters"=c(nchar_blogs,nchar_news,nchar_twitter),
                  "Number of words"=c(nword_blogs,nword_news,nword_twitter))
table
```

## Clean data

```{r clean data}
set.seed(12345)
blogs1<-iconv(blogs,"latin1","ASCII",sub="")
news1<-iconv(news,"latin1","ASCII",sub="")
twitter1<-iconv(twitter,"latin1","ASCII",sub="")
rm(blogs)
rm(news)
rm(twitter)

# sample data set only 1% of each file
sample_data<-c(sample(blogs1,length(blogs1)*0.01),
               sample(news1,length(news1)*0.01),
               sample(twitter1,length(twitter1)*0.01))
rm(blogs1)
rm(news1)
rm(twitter1)
```

Datasets are very large so used Sample function to use 1% of each file,


## Building corpus
```{r build corpus}
library(tm)
library(NLP)
corpus<-VCorpus(VectorSource(sample_data))
corpus1<-tm_map(corpus,removePunctuation)
corpus2<-tm_map(corpus1,stripWhitespace)
corpus3<-tm_map(corpus2,tolower)
corpus4<-tm_map(corpus3,removeNumbers)
corpus5<-tm_map(corpus4,PlainTextDocument)
corpus6<-tm_map(corpus5,removeWords,stopwords("english"))
corpus_result<-data.frame(text=unlist(sapply(corpus6,'[',"content")),stringsAsFactors = FALSE)
head(corpus_result)

rm(corpus)
rm(corpus1)
rm(corpus2)
rm(corpus3)
rm(corpus4)
rm(corpus5)
```

Corpus have been built. Now, creating Data frames for verifying corpus.

## Building N-gram

```{r build N-gram}
library(RWeka)
one<-function(x) NGramTokenizer(x,Weka_control(min=1,max=1))
two<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
thr<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
one_table<-TermDocumentMatrix(corpus6,control=list(tokenize=one))
two_table<-TermDocumentMatrix(corpus6,control=list(tokenize=two))
thr_table<-TermDocumentMatrix(corpus6,control=list(tokenize=thr))
one_corpus<-findFreqTerms(one_table,lowfreq=1000)
two_corpus<-findFreqTerms(two_table,lowfreq=80)
thr_corpus<-findFreqTerms(thr_table,lowfreq=10)
one_corpus_num<-rowSums(as.matrix(one_table[one_corpus,]))
one_corpus_table<-data.frame(Word=names(one_corpus_num),frequency=one_corpus_num)
one_corpus_sort<-one_corpus_table[order(-one_corpus_table$frequency),]
head(one_corpus_sort)
two_corpus_num<-rowSums(as.matrix(two_table[two_corpus,]))
two_corpus_table<-data.frame(Word=names(two_corpus_num),frequency=two_corpus_num)
two_corpus_sort<-two_corpus_table[order(-two_corpus_table$frequency),]
head(two_corpus_sort)
thr_corpus_num<-rowSums(as.matrix(thr_table[thr_corpus,]))
thr_corpus_table<-data.frame(Word=names(thr_corpus_num),frequency=thr_corpus_num)
thr_corpus_sort<-thr_corpus_table[order(-thr_corpus_table$frequency),]
head(thr_corpus_sort)
```

This extracts words and frequencies of N-grams.

## Plot graph

```{r plot}
library(ggplot2)
one_g<-ggplot(one_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
one_g<-one_g+geom_bar(stat="identity")
one_g<-one_g+labs(title="Unigrams",x="Words",y="Frequency")
one_g<-one_g+theme(axis.text.x=element_text(angle=90))
one_g
two_g<-ggplot(two_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
two_g<-two_g+geom_bar(stat="identity")
two_g<-two_g+labs(title="Bigrams",x="Words",y="Frequency")
two_g<-two_g+theme(axis.text.x=element_text(angle=90))
two_g
thr_g<-ggplot(thr_corpus_sort[1:10,],aes(x=reorder(Word,-frequency),y=frequency,fill=frequency))
thr_g<-thr_g+geom_bar(stat="identity")
thr_g<-thr_g+labs(title="Trigrams",x="Words",y="Frequency")
thr_g<-thr_g+theme(axis.text.x=element_text(angle=90))
thr_g
```
Now, graphs have been plotted for each N-gram word. This can be confirmed which word is the most frequently used in those files.

## Next plans

Here, I have performed initial analysis on given dataset. I shall reevaluate current approach and check if  - sample size can be adjusted using diffrent cleansing techniques
- punctuation, numbers etc. to improve prediction.
improve Prediction algorithm and build a predictive model